{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from tqdm.notebook import tqdm \n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"127.0.0.1:27017\")\n",
    "db = client['IRsegmentationDB3']\n",
    "\n",
    "# This dataset contains documents treated by Annotation Correction, Annotation Filtering and \n",
    "# non ascii char removing\n",
    "dataset = db['dataset']\n",
    "\n",
    "# This dataset contains the documents (by dataset) segmented in paragraph based on annotations\n",
    "# This dataset is used only for Supervised Learning \n",
    "pDataset = db['pDataset']\n",
    "\n",
    "# This dataset contains the documents split in tokens. This token are filtered to make sure that\n",
    "# they are not punctuations and that are lowercase. \n",
    "uDict = db['unsDict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./Dataset/trade_secret_cases.json')\n",
    "data = json.load(f)\n",
    "docs = [d for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(docs):\n",
    "    # Sorting annotation by 'start' index \n",
    "    annots = sorted(x['annotations'], key=lambda d: d['start']) \n",
    "    \n",
    "    # Annotations correction\n",
    "    for j in range(0, len(annots) - 1):\n",
    "        if(annots[j]['end'] == annots[j+1]['start']):\n",
    "            annots[j+1]['start'] +=1 \n",
    "        elif(annots[j]['end'] > annots[j+1]['start']):\n",
    "            annots[j+1]['start'] = annots[j]['end'] + 1\n",
    "            \n",
    "    x['annotations'] = annots\n",
    "    \n",
    "    # Remove non ASCII characters\n",
    "    x['text'] = re.sub(r'[^\\x00-\\x7F]+',' ', x['text'])\n",
    "    \n",
    "    # Segmenting by annotations\n",
    "    mp = [x['text'][a['start']:a['end']] for a in annots]\n",
    "    mpf, anf = [], []\n",
    "    \n",
    "    # Remove Appendix and Dissent / Concurrence paragraphs from documnets and annotations\n",
    "    j = 0\n",
    "    for kk, (p, a) in enumerate(zip(mp, annots)):\n",
    "        if a['type'] != \"Appendix\" and a['type'] != \"Dissent/Concurrence\" and len(p) > 0:\n",
    "            a['index'] = j\n",
    "            mpf.append(p)\n",
    "            anf.append(a)\n",
    "            j += 1\n",
    "        \n",
    "        \n",
    "    mpf = \"¶\".join(mpf)\n",
    "    mpf = x['text'][0:annots[0]['start']]+str(mpf)\n",
    "    \n",
    "    x['text'] = mpf\n",
    "    x['doc'] = i\n",
    "    x['annotations'] = anf\n",
    "    \n",
    "    dataset.insert_one(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(dataset.find()): \n",
    "    sent = []\n",
    "    \n",
    "    d = (x['text']).split(\"¶\")\n",
    "\n",
    "    for j, p in enumerate(d): \n",
    "        s = sent_tokenize(p, language=\"english\")\n",
    "        s = [snt.lower() for snt in s]\n",
    "        \n",
    "        if(len(s) == 0):\n",
    "            continue \n",
    "    \n",
    "        # mergin too small sentence\n",
    "        k = 0\n",
    "        \n",
    "        while(True):\n",
    "            if(len(s) - 1 == k):\n",
    "                break\n",
    "            \n",
    "            if(len(nltk.word_tokenize(s[k])) <= 5):\n",
    "                s[k - 1] = s[k - 1]+\" \"+s[k]\n",
    "                del s[k]\n",
    "            else:\n",
    "                k += 1\n",
    "                \n",
    "        sent.append(s)\n",
    "        \n",
    "    x['text'] = sent\n",
    "    \n",
    "    pDataset.insert_one(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(dataset.find()):\n",
    "    text = d['text']\n",
    "    tokens = nltk.word_tokenize(text)   \n",
    "    tokens = [w.lower().strip() for w in tokens ]\n",
    "    tokens = [x.replace(\"¶\", \"\") for x in tokens if x not in punctuations]\n",
    "    \n",
    "    uDict.insert_one({\"doc\": i, \"text\": tokens, \"title\": d['name']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
