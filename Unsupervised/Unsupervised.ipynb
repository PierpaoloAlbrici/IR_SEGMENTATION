{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unseripervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import defaultdict\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"127.0.0.1:27017\")\n",
    "\n",
    "db = client['IRsegmentationDB3']\n",
    "\n",
    "# Dataset Documenti\n",
    "docsDataset = db['dataset']\n",
    "\n",
    "# Dataset Information Content\n",
    "icDataset = db['unsIC']\n",
    "\n",
    "#relDataset contains the relatedness score for each sentence combination in each doc\n",
    "relDataset = db['relatedness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedClassification: \n",
    "    def __init__(self, n, t):\n",
    "        self.docs = None\n",
    "        self.annot = None\n",
    "        self.wordDict = None\n",
    "        self.S = []\n",
    "        self.DOC = None\n",
    "        self.g = nx.Graph()\n",
    "        self.cliques = []\n",
    "        self.SG = []\n",
    "        self.w2v_keys = None\n",
    "        self.w2v_model = None\n",
    "        self.n = n\n",
    "        self.t = t\n",
    "        \n",
    "        print(\"\\n\\nUNSUPERVISED MODULE - INITIALIZATION\")\n",
    "        print(\"N: \", self.n)\n",
    "        \n",
    "    def import_dataset(self):\n",
    "        data = docsDataset.find()\n",
    "        self.docs = [(d['text']).replace(\"Â¶\", \" \") for d in data]\n",
    "        self.annot = [d['annotations'] for d in data]\n",
    "\n",
    "        self.wordDict = [x['word'] for x in icDataset.find()]\n",
    "        \n",
    "        #Pre trained Word to Vector Model\n",
    "        self.w2v_model = Word2Vec.load(\"./w2v_hyb3.model\")\n",
    "        self.w2v_keys = list(self.w2v_model.wv.key_to_index.keys())\n",
    "        \n",
    "        \n",
    "    def sentence_tokenizer(self):\n",
    "        self.S = [sent_tokenize(d, language='english') for d in self.docs]\n",
    "        \n",
    "        # mergin too small sentence\n",
    "        i = 0\n",
    "        while(True):\n",
    "            if(len(self.S) - 1 == i):\n",
    "                break\n",
    "                \n",
    "            j = 1\n",
    "            while(True):\n",
    "                if(j > len(self.S[i]) - 1):\n",
    "                    break\n",
    "                    \n",
    "                if(len(nltk.word_tokenize(self.S[i][j])) <= 3):\n",
    "                    self.S[i][j - 1] = self.S[i][j - 1]+\" \"+self.S[i][j]\n",
    "                    del self.S[i][j]\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "        \n",
    "        print(\"Documents tokenized\")\n",
    "    \n",
    "    def compute_relatedness(self, doc):\n",
    "        self.DOC = doc\n",
    "        \n",
    "        print(\"DOC: \", self.DOC)\n",
    "        print(\"LENGTH SENTENCES \", len(self.S[self.DOC]))\n",
    "        \n",
    "        if(relDataset.find_one({'doc': self.DOC})):\n",
    "            print(\"Document \"+str(self.DOC)+\" already in DB. No relatedness computation needed.\")\n",
    "        else:\n",
    "            # Constructing the similarity graph\n",
    "            iterations = list(combinations(list(range(len(self.S[self.DOC]))), 2))\n",
    "\n",
    "            for i, s1 in tqdm(enumerate(self.S[self.DOC])):\n",
    "                relVal = []\n",
    "\n",
    "                for j, s2 in enumerate(self.S[self.DOC]):\n",
    "                    if(i == j):\n",
    "                        continue\n",
    "                        \n",
    "                    if(abs(i - j) != 1):\n",
    "                        continue\n",
    "\n",
    "                    if(i, j) not in iterations:\n",
    "                        # Ottimizzazione\n",
    "                        for rd in relDataset.find({\"doc\": self.DOC, \"s1\": str(self.DOC)+\"_\"+str(j)}): \n",
    "                            for r in rd['rel']:\n",
    "                                if(r['s2'] == str(self.DOC)+\"_\"+str(i)):\n",
    "                                    relVal.append({\"s2\": str(self.DOC)+\"_\"+str(j), \"value\": r['value']})\n",
    "                    else:\n",
    "                        st1 = [x.lower().strip() for x in nltk.word_tokenize(s1) if x not in punctuations]\n",
    "                        st2 = [x.lower().strip() for x in nltk.word_tokenize(s2) if x not in punctuations]\n",
    "\n",
    "                        if(len(st1) == 0 or len(st2) == 0):\n",
    "                            continue\n",
    "\n",
    "                        crel = self.relatedness(st1, st2)\n",
    "                        relVal.append({\"s2\": str(self.DOC)+\"_\"+str(j), \"value\": crel})\n",
    "\n",
    "                relDataset.insert_one({\"doc\": self.DOC, \"s1\": str(self.DOC)+\"_\"+str(i), \"rel\": relVal})\n",
    "            \n",
    "    def relatedness(self, st1, st2):\n",
    "        sr = 0\n",
    "\n",
    "        # Compute SR\n",
    "        lst1 = 0\n",
    "        for ws1 in st1: \n",
    "            lst1 += 1\n",
    "            if(ws1 not in self.w2v_keys):\n",
    "                continue\n",
    "\n",
    "            lst2 = 0\n",
    "            \n",
    "            ic_ws1 = icDataset.find_one({\"word\": ws1})['ic']\n",
    "            sr_ws1 = np.reshape([float(wi) for wi in (list(self.w2v_model.wv[ws1]))], (1, -1))\n",
    "\n",
    "            for ws2 in st2:\n",
    "                lst2 += 1\n",
    "                if(ws1 in self.wordDict and ws2 in self.wordDict and ws2 in self.w2v_keys):\n",
    "                    sr_ws2 = [float(wi) for wi in (list(self.w2v_model.wv[ws2]))]\n",
    "\n",
    "                    cs = cosine_similarity(sr_ws1, np.reshape(sr_ws2, (1, -1)))[0][0]\n",
    "\n",
    "                    ic_ws2 = icDataset.find_one({\"word\": ws2})['ic']\n",
    "\n",
    "                    min_ic = min(ic_ws1, ic_ws2)\n",
    "                    sr += (cs * min_ic)\n",
    "                else: \n",
    "                    sr += 0\n",
    "        \n",
    "        fm = sr / lst1\n",
    "        sm = sr / lst2\n",
    "        res = (fm + sm) / 2\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def rel_ths_avg_std(self):\n",
    "        relScore = []\n",
    "        c = 0\n",
    "        for rel in relDataset.find({\"doc\": self.DOC}):\n",
    "            for item in rel['rel']:\n",
    "                if(item['value'] != 0.0):\n",
    "                    relScore.append(float(item['value']))\n",
    "                    c += 1\n",
    "\n",
    "        std = round(np.std(relScore), 2)\n",
    "        avg = round((sum(relScore) / c), 2)\n",
    "        return avg, std\n",
    "\n",
    "    def filtering(self):\n",
    "        for i, s in enumerate(self.S[self.DOC]):\n",
    "            self.g.add_node(i)\n",
    "\n",
    "        ### Apply filtering to relatendess\n",
    "        relValList = []\n",
    "        t_avg, t_std = self.rel_ths_avg_std()\n",
    "        \n",
    "        if(self.t == 1):\n",
    "            print(\"T = AVG - STD\")\n",
    "            self.t = round((t_avg - t_std), 2)\n",
    "        else:\n",
    "            print(\"T = AVG\")\n",
    "            self.t = round((t_avg), 2) \n",
    "        \n",
    "        for rel in relDataset.find({\"doc\": self.DOC}):\n",
    "            s1 = rel['s1'].split(\"_\")\n",
    "            for j, item in enumerate(rel['rel']):\n",
    "                s2 = item['s2'].split(\"_\")\n",
    "                val = item['value']\n",
    "\n",
    "                if(val == 0.0):\n",
    "                    continue \n",
    "\n",
    "                relValList.append(val)\n",
    "               \n",
    "                if(val > self.t):\n",
    "                    self.g.add_edge(int(s1[1]), int(s2[1]), rel = val)\n",
    "                    \n",
    "    def set_cliques(self):\n",
    "        for x in nx.find_cliques(self.g):\n",
    "            xs = sorted(x)\n",
    "            self.cliques.append(xs)\n",
    "        #print(\"\\n\", len(self.cliques), \" cliques find\")\n",
    "    \n",
    "    ##### Creating initial Segments from Cliques\n",
    "    def find_nested(self, s):\n",
    "        for sg in self.SG: \n",
    "            if(type(sg) == list):\n",
    "                for sub in sg:\n",
    "                    if(s == sub):\n",
    "                        return True\n",
    "            elif(s in self.SG):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def find_nested_pos(self, s):\n",
    "        for i, sg in enumerate(self.SG): \n",
    "            if(type(sg) == list):\n",
    "                for sub in sg:\n",
    "                    if(s == sub):\n",
    "                        return i\n",
    "            elif(s in self.SG):\n",
    "                 return sg.index(s)\n",
    "            \n",
    "        return -1\n",
    "\n",
    "    def initial_segments(self):\n",
    "        print(\"Computing Initial Segments ... \")\n",
    "        for i, q in enumerate(self.cliques): \n",
    "            for n1, n2 in list(combinations(q, 2)):\n",
    "                if(n1 == n2):\n",
    "                    continue\n",
    "\n",
    "                if((n2 - n1) == 1):\n",
    "                    if(self.find_nested(n1) == False and self.find_nested(n2) == False):\n",
    "                        self.SG.append([n1])\n",
    "                        self.SG.append([n2])\n",
    "                    elif(self.find_nested(n1) == True and self.find_nested(n2) == False):\n",
    "                        self.SG[self.find_nested_pos(n1)].append(n2)\n",
    "                    elif(self.find_nested(n1) == False and self.find_nested(n2) == True):\n",
    "                        self.SG[self.find_nested_pos(n2)].append(n1)\n",
    "        \n",
    "        self.sorting_SG()\n",
    "        \n",
    "    def sorting_SG(self):\n",
    "        for i, sg in enumerate(self.SG): \n",
    "            if(type(self.SG[i]) != list):\n",
    "                self.SG[i] = [self.SG[i]]\n",
    "            self.SG[i].sort()\n",
    "        self.SG = sorted(self.SG, key=lambda x:int(x[0]))\n",
    "\n",
    "    def merging_adj(self):\n",
    "        # Merging Adjacent Segments \n",
    "        print(\"Merging Adjacent Segments ... \")\n",
    "        \n",
    "        i = 0\n",
    "        while(True):\n",
    "            if(i < len(self.SG) - 1):\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            qc = 0\n",
    "            qf = True\n",
    "            \n",
    "            while(qf == True):\n",
    "                if(qc == len(self.cliques) - 1):\n",
    "                    qf = False\n",
    "\n",
    "                q = self.cliques[qc]\n",
    "                for n1, n2 in list(combinations(q, 2)):\n",
    "                    if(n1 in self.SG[i] and n2 in self.SG[i+1]):\n",
    "                        sg1 = self.SG[i]\n",
    "                        sg2 = self.SG[i+1]\n",
    "\n",
    "                        self.SG[i] = sg1 + sg2\n",
    "                        del self.SG[i+1]\n",
    "\n",
    "                        qf = False\n",
    "                        break\n",
    "                qc += 1\n",
    "            i += 1\n",
    "    \n",
    "    def get_relatedness_from_DB(self, s1):\n",
    "        res =  relDataset.find_one({\"doc\": self.DOC, \"s1\": str(self.DOC)+\"_\"+str(s1)})\n",
    "        \n",
    "        rr = defaultdict(lambda: 0)\n",
    "        \n",
    "        for r in res['rel']: \n",
    "            rr[r['s2']] = r['value']\n",
    "            \n",
    "        return rr\n",
    "    \n",
    "    def compute_sgr(self, sg1, sg2):\n",
    "        rel = 0\n",
    "        for n1 in sg1:  \n",
    "            st1 = [x.lower().strip() for x in nltk.word_tokenize(self.S[self.DOC][int(n1)]) if x not in punctuations]\n",
    "            relDict = self.get_relatedness_from_DB(n1)\n",
    "            \n",
    "            for n2 in sg2: \n",
    "                st2 = [x.lower().strip() for x in nltk.word_tokenize(self.S[self.DOC][int(n2)]) if x not in punctuations]\n",
    "                rel += int(relDict[n2])\n",
    "\n",
    "        return (1 / (len(sg1) * len(sg2))) * rel\n",
    "    \n",
    "    def merging_small_segments(self):\n",
    "        print(\"Merging Smal Segments ... \")\n",
    "        \n",
    "        i = 0\n",
    "        while(True):\n",
    "            if(i < len(self.SG) - 1):\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            if(len(self.SG[i]) < self.n):\n",
    "                if(i == 0):\n",
    "                    sg1 = self.SG[i]\n",
    "                    sg2 = self.SG[i+1]\n",
    "\n",
    "                    self.SG[i] = sg1 + sg2\n",
    "                    del self.SG[i + 1]\n",
    "                else:\n",
    "                    if(self.compute_sgr(self.SG[i-1], self.SG[i]) > self.compute_sgr(self.SG[i], self.SG[i+1])):\n",
    "                        sg1 = self.SG[i - 1]\n",
    "                        sg2 = self.SG[i]\n",
    "\n",
    "                        self.SG[i - 1] = sg1 + sg2\n",
    "\n",
    "                        del self.SG[i]\n",
    "                    else:\n",
    "                        sg1 = self.SG[i]\n",
    "                        sg2 = self.SG[i+1]\n",
    "\n",
    "                        self.SG[i] = sg1 + sg2\n",
    "                        del self.SG[i + 1]\n",
    "            else: \n",
    "                i += 1\n",
    "\n",
    "        self.sorting_SG()\n",
    "            \n",
    "    def get_SG(self):\n",
    "        return self.SG\n",
    "    \n",
    "    def get_S(self, DOC):\n",
    "        return self.S[DOC]\n",
    "    \n",
    "    def save(self):\n",
    "        item = {\"doc\": self.DOC, \"t\": self.t, \"SG\": self.SG}\n",
    "        resDataset.insert_one(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unsupervised(DOC, n, t):\n",
    "    # Instantiate the Unserpvised Classificator\n",
    "    # Params n,t  -> n is the minimum size for a segment and 1 is the threshold value (this value is override\n",
    "    # in run time)\n",
    "    cUns = UnsupervisedClassification(n, t) #0 -> avg  -- 1 -> avg  - std \n",
    "    cUns.import_dataset()\n",
    "    cUns.sentence_tokenizer()\n",
    "    \n",
    "    cUns.compute_relatedness(DOC)\n",
    "    \n",
    "    # Graph's edges filtering based on the threshold value\n",
    "    cUns.filtering()\n",
    " \n",
    "    # Cliques computation\n",
    "    cUns.set_cliques()\n",
    "\n",
    "    # Segments\n",
    "    cUns.initial_segments()\n",
    "    cUns.sorting_SG()\n",
    "    print(cUns.get_SG())\n",
    "    \n",
    "    # Merging adjacent segments (if related)\n",
    "    cUns.merging_adj()\n",
    "    print(cUns.get_SG())\n",
    "    # Merging too small segments (based on the parameter n). \n",
    "    cUns.merging_small_segments()\n",
    "    print(cUns.get_SG())\n",
    "    #cUns.sorting_SG()\n",
    "    \n",
    "    # Save resulting SG\n",
    "    #cUns.save()\n",
    "    \n",
    "    return cUns.get_SG(), cUns.get_S(DOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unsupervised(0, 7, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
