{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%run Unsupervised.ipynb\n",
    "%run Analyzers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC = 0\n",
    "n = 13\n",
    "t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"127.0.0.1:27017\")\n",
    "\n",
    "db = client['IRsegmentationDB3']\n",
    "docsDataset = db['dataset']\n",
    "\n",
    "#relDataset contains the relatedness score for each sentence combination in each doc\n",
    "relDataset = db['relatedness']\n",
    "\n",
    "item = docsDataset.find_one({'doc': DOC})\n",
    "annot = item['annotations']\n",
    "doc = item['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupervisedLabeling(SG, S, DOC):\n",
    "    res = []\n",
    "\n",
    "    # Recomposing the document based on SG\n",
    "    segments = []\n",
    "    \n",
    "    for sg in SG: \n",
    "        segment = []\n",
    "        for snt in sg: \n",
    "            segment.append(S[int(snt)])\n",
    "        segments.append(segment)\n",
    "        \n",
    "    fx = FeatureExtraction(segments, DOC)\n",
    "    fv = fx.get_feature_vector()\n",
    "    \n",
    "    # Functional Part Analyzer\n",
    "    fpa = functionalPartAnalyzer(segments, fv, DOC)\n",
    "    fpa.introduction()\n",
    "    fpa.background()\n",
    "    fpa.footnotes()\n",
    "\n",
    "    res = fpa.getClassification()\n",
    "\n",
    "    # Conclusion Recognizer\n",
    "    idx = [(x[1].split(\"=\"))[1] for x in fpa.getFilteredFeatureVector()]\n",
    "    \n",
    "    for x in idx: \n",
    "        pfv = fx.get_feature_vector_for_sentence(x)\n",
    "        cr = conclusionRecognizer(segments[int(x)], x, pfv, DOC)\n",
    "        res.append(cr.recognizer())\n",
    "\n",
    "    res = sorted(res, key=lambda d: int(d['index'])) \n",
    "    \n",
    "    # Check if there are mutliple paragraph with the same labels consequently\n",
    "    # if true, merge them. \n",
    "    labeling = []\n",
    "    for i, r in enumerate(res):\n",
    "        if(i > 0):\n",
    "            if(r['label'] == labeling[len(labeling) - 1]['label']):\n",
    "                labeling[len(labeling) - 1]['index'].extend([r['index']])\n",
    "                continue\n",
    "        r['index'] = [r['index']]\n",
    "        labeling.append(r)\n",
    "    \n",
    "    return labeling, S, SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG, S = Unsupervised(DOC, n, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = SupervisedLabeling(SG, S, DOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if two sentences are in the same segments\n",
    "def isInTheSameSegment(ns1, ns2, SG):\n",
    "    for sg in SG: \n",
    "        if(ns1 in sg and ns2 in sg):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def isInTheSameGoldSegment(d, s1, s2, S):\n",
    "    doc = docsDataset.find_one({'doc': d})\n",
    "    text = doc['text'].split(\"¶\")\n",
    "        \n",
    "    for p in text:\n",
    "        p = p.replace(\"\\n\", \"\").strip()\n",
    "        \n",
    "        if(p.count(s1) > 0 and p.count(s2) > 0):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def unsupervisedEvaluation(d, k, SG, S):\n",
    "    counter = 0\n",
    "    for i in range(0, 100):\n",
    "        n = random.randrange(0, len(S) - k)\n",
    "\n",
    "        s1, ns1 = S[n], n\n",
    "        s2, ns2 = S[n + k], n + k\n",
    "        \n",
    "        iss = isInTheSameSegment(ns1, ns2, SG)\n",
    "        isgs = isInTheSameGoldSegment(d, s1, s2, S)\n",
    "    \n",
    "        if(iss == isgs):\n",
    "            counter += 1\n",
    "            \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisedEvaluation(d, r, k):\n",
    "    c = []\n",
    "    for i in range(0, 100):\n",
    "        c.append(haveSameLabel(d, r, k))\n",
    "        \n",
    "    return c\n",
    "\n",
    "def haveSameLabel(d, r, k): \n",
    "    doc = docsDataset.find_one({'doc': d})\n",
    "    annots = docsDataset.find_one({'doc': d})['annotations']\n",
    "    text = doc['text'].split(\"¶\")\n",
    "    \n",
    "    S = r[1]\n",
    "    n = random.randrange(0, len(S) - k)\n",
    "    \n",
    "    s1, ns1 = S[n], n\n",
    "    s2, ns2 = S[n + k], n + k\n",
    "\n",
    "    # Get Predicted Labels\n",
    "    pLabel1 = getLabel(ns1, r)\n",
    "    #pLabel2 = getLabel(ns2, r)\n",
    "    \n",
    "    if(pLabel1 == \"NF\"): #or pLabel2 == \"NF\"):\n",
    "        return 0\n",
    "\n",
    "    # Get real labels\n",
    "    rLabel1, rLabel2 = \"NF\", \"NF\"\n",
    "    \n",
    "    for p in zip(text, annots):\n",
    "        prg = p[0].replace(\"\\n\", \"\").strip()\n",
    "        if(prg.count(s1) > 0):\n",
    "            rLabel1 = p[1]['type']\n",
    "            \n",
    "            if(p[1]['type'] == 'Analysis'):\n",
    "                rLabel1 = 'Conclusions'\n",
    "            \n",
    "        if(prg.count(s2) > 0):\n",
    "            rLabel2 = p[1]['type']\n",
    "            \n",
    "    if(rLabel1 == \"NF\"):# or rLabel2 == \"NF\"):\n",
    "        return 0\n",
    "    \n",
    "    if(pLabel1 == rLabel1):\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def getLabel(ns, r):\n",
    "    #ciclo SG\n",
    "    for i, rr in enumerate(r[2]):\n",
    "        # Trovo indice segmento in cui è presente la mia frase\n",
    "        if(ns in rr):\n",
    "            # ciclo i segmenti finali aggregati\n",
    "            for idx in r[0]:\n",
    "                # se il segmento è presente nell'aggregato ritorno l'etichetta\n",
    "                if(i in idx['index']):\n",
    "                    if(idx['label'] == 'Analysis'):\n",
    "                        return \"Conclusions\"\n",
    "                    \n",
    "                    return idx['label']\n",
    "        \n",
    "    return \"NF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UE = []\n",
    "SU = []\n",
    "DOCS = set(sorted([r['doc'] for r in relDataset.find()]))\n",
    "print(DOCS)\n",
    "\n",
    "for d in DOCS:\n",
    "    annots = docsDataset.find_one({'doc': d})['annotations']\n",
    "    \n",
    "    # Unsupervised\n",
    "    SG, S = Unsupervised(d, n, t)\n",
    "     \n",
    "    #Evaluation \n",
    "    # k =  distanza tra segmenti\n",
    "    k = round(len(SG) / len(annots))\n",
    "    \n",
    "    UE.append((d, unsupervisedEvaluation(d, k, SG, S)))\n",
    "    \n",
    "    r = SupervisedLabeling(SG, S, d)\n",
    "    SU.append((d, supervisedEvaluation(d, r, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for su in SU:\n",
    "    \n",
    "    print(\"DOC\", su[0])\n",
    "    acc += sum(su[1])\n",
    "    \n",
    "print(\"ACCURATEZZA SUPERVISED\", round((acc * 100) / (len(SU) * 100), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for su in UE:\n",
    "    \n",
    "    print(\"DOC\", su[0])\n",
    "    print(su[1])\n",
    "    acc += su[1]\n",
    "    \n",
    "print(\"ACCURATEZZA UNSUPERVISED\", round((acc * 100) / (len(SU) * 100), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
