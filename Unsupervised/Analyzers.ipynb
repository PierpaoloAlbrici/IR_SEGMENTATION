{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class functionalPartAnalyzer:\n",
    "    def __init__(self, doc, fv, idx):\n",
    "        self.doc = doc\n",
    "        self.fv = fv\n",
    "        self.index = idx\n",
    "        self.ffv = []\n",
    "        self.classification = []\n",
    "    \n",
    "    def introduction(self):\n",
    "        self.ffv = []\n",
    "        \n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(\"../Supervised/Models/intro.model\")\n",
    "        \n",
    "        y_pred = tagger.tag(self.fv)\n",
    "        \n",
    "        # Labeling\n",
    "        for i, (y, fv) in enumerate(zip(y_pred, self.fv)):\n",
    "            if(y == \"1\"):\n",
    "                self.classification.append({\n",
    "                    \"doc\": int(self.index),\n",
    "                    \"index\": int(fv[1].split(\"=\")[1]),\n",
    "                    \"label\": \"Introduction\"\n",
    "                })\n",
    "            else: \n",
    "                self.ffv.append(fv)\n",
    "                \n",
    "    def background(self):\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(\"../Supervised/Models/background.model\")\n",
    "        \n",
    "        y_pred = tagger.tag(self.ffv)\n",
    "        \n",
    "        # Labeling\n",
    "        tffv = []\n",
    "        for i, (y, fv) in enumerate(zip(y_pred, self.ffv)):\n",
    "            if(y == \"1\"):\n",
    "                self.classification.append({\n",
    "                    \"doc\": int(self.index), \n",
    "                    \"index\": int(fv[1].split(\"=\")[1]),\n",
    "                    \"label\": \"Background\"\n",
    "                })\n",
    "            else: \n",
    "                tffv.append(fv)\n",
    "        self.ffv = tffv\n",
    "                \n",
    "    def footnotes(self):\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(\"../Supervised/Models/footnotes.model\")\n",
    "        \n",
    "        y_pred = tagger.tag(self.ffv)\n",
    "        \n",
    "        # Labeling\n",
    "        tffv = []\n",
    "        for i, (y, fv) in enumerate(zip(y_pred, self.ffv)):\n",
    "            if(y == \"1\"):\n",
    "                self.classification.append({\n",
    "                    \"doc\": int(self.index),\n",
    "                    \"index\": int(fv[1].split(\"=\")[1]),\n",
    "                    \"label\": \"Footnotes\"\n",
    "                })\n",
    "            else:\n",
    "                tffv.append(fv)\n",
    "            \n",
    "        self.ffv = tffv\n",
    "        \n",
    "    def getClassification(self):\n",
    "        return self.classification\n",
    "        \n",
    "    def getFilteredFeatureVector(self):\n",
    "        return self.ffv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conclusionRecognizer:\n",
    "    def __init__(self, p, ip, fv, idx):\n",
    "        self.paragraph = p\n",
    "        self.iParagraph = ip\n",
    "        self.index = idx\n",
    "        self.fv = fv\n",
    "        self.classification = None\n",
    "        \n",
    "    def recognizer(self): \n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(\"../Supervised/Models/conclusion.model\")\n",
    "        y_pred = tagger.tag(self.fv) \n",
    "        \n",
    "        if(sum([int(y) for y in y_pred]) > (len(y_pred) / 2)):\n",
    "            self.classification = {\n",
    "                \"doc\": int(self.index),\n",
    "                \"index\": int(self.iParagraph),\n",
    "                \"label\": \"Analysis\"\n",
    "            }\n",
    "        else:\n",
    "            self.classification = {\n",
    "                \"doc\": int(self.index),\n",
    "                \"index\": int(self.iParagraph),\n",
    "                \"label\": \"Conclusions\"\n",
    "            }\n",
    "            \n",
    "        return self.classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    def __init__(self, doc, i):\n",
    "        self.doc = doc.copy()\n",
    "        self.index = i\n",
    "        \n",
    "    def get_feature_vector(self):\n",
    "        fvs = []\n",
    "        tdoc = []\n",
    "        \n",
    "        for j, p in enumerate(self.doc):\n",
    "            l = []\n",
    "            for k, s in enumerate(p):\n",
    "                l.append(word_tokenize(s))\n",
    "            tdoc.append(l)\n",
    "            \n",
    "        for j, p in enumerate(tdoc):\n",
    "            fv = [\n",
    "                f\"paragraph.doc={self.index}\",\n",
    "                f\"paragraph.position={j}\",\n",
    "                f\"paragraph.length={self.paragraph_length(p)}\",\n",
    "                f\"paragraph.average_sentence_length={self.avg_sentence_length(p)}\"\n",
    "            ]\n",
    "            \n",
    "            # first five tokens (and pos) in paragraph\n",
    "            n = 5\n",
    "            if(len(p[0]) < 5):\n",
    "                n = len(p[0])\n",
    "                \n",
    "            ftokens_pos = self.get_first_pos(p[0], n)\n",
    "            for k, ps in enumerate(ftokens_pos):\n",
    "                fv.extend([\n",
    "                    f\"paragraph.first_word[+{k}]={ps[0]}\",\n",
    "                    f\"paragraph.first_pos[+{k}]={ps[1]}\"\n",
    "                ])\n",
    "            \n",
    "            # last five tokens (and pos) in paragraph\n",
    "            n = 5\n",
    "            if(len(p[len(p) - 1]) < 5): \n",
    "                n = len(p[len(p) - 1])\n",
    "                \n",
    "            ltokens_pos = self.get_last_pos(p[len(p) - 1], n)\n",
    "            for k, ps in enumerate(ltokens_pos):\n",
    "                fv.extend([\n",
    "                    f\"paragraph.last_word[+{k}]={ps[0]}\",\n",
    "                    f\"paragraph.last_pos[+{k}]={ps[1]}\"\n",
    "                ])\n",
    "            \n",
    "            if j > 0 :\n",
    "                fv.extend([\n",
    "                    f\"paragraph.prev_paragraph_length={self.prev_paragraph_length(j)}\"\n",
    "                ])\n",
    "            else:\n",
    "                fv.extend([\"BOD\"])\n",
    "                \n",
    "            # First two tokenk in each sentence\n",
    "            for k, s in enumerate(p): \n",
    "                wpos = self.get_first_pos(s, 2)\n",
    "                for kk, (w, p) in enumerate(zip(s[:2], wpos)):\n",
    "                    fv.extend([\n",
    "                        f\"paragraph.sentence[+{k}].word[+{kk}]={w}\",\n",
    "                        f\"paragraph.sentence[+{k}].word_pos[+{kk}]={p[1]}\",\n",
    "                        f\"paragraph.sentence[+{k}].word_lower[+{kk}]={w.lower()}\",\n",
    "                        f\"paragraph.sentence[+{k}].isupper[+{kk}]={w.isupper()}\",\n",
    "                        f\"paragraph.sentence[+{k}].istitle[+{kk}]={w.istitle()}\",\n",
    "                        f\"paragraph.sentence[+{k}].isdigit[+{kk}]={w.isdigit()}\"\n",
    "                    ])\n",
    "                \n",
    "            # Last tow tokens in each sentence\n",
    "            for k, s in enumerate(p): \n",
    "                wpos = self.get_last_pos(s, 2)\n",
    "                for kk, (w, p) in enumerate(zip(s[-2:], wpos)):\n",
    "                    fv.extend([\n",
    "                        f\"paragraph.sentence[+{k}].word[-{kk}]={w}\",\n",
    "                        f\"paragraph.sentence[+{k}].word_post[-{kk}]={p[1]}\",\n",
    "                        f\"paragraph.sentence[+{k}].word_lower[-{kk}]={w.lower()}\",\n",
    "                        f\"paragraph.sentence[+{k}].isupper[-{kk}]={w.isupper()}\",\n",
    "                        f\"paragraph.sentence[+{k}].istitle[-{kk}]={w.istitle()}\",\n",
    "                        f\"paragraph.sentence[+{k}].isdigit[-{kk}]={w.isdigit()}\"\n",
    "                    ])\n",
    "                \n",
    "            if j < len(self.doc) - 1:\n",
    "                 fv.extend([\n",
    "                     f\"paragraph.next_paragraph_length={self.next_paragraph_length(j)}\"\n",
    "                 ])\n",
    "            else:\n",
    "                fv.extend([\"EOD\"])\n",
    " \n",
    "            fvs.append(fv)\n",
    "\n",
    "        return fvs\n",
    "\n",
    "    # Used during the training of Conclusion Recognizer\n",
    "\n",
    "    def get_feature_vector_for_sentences(self):\n",
    "        fvp = []\n",
    "        tdoc = []\n",
    "        \n",
    "        for j, p in enumerate(self.doc):\n",
    "            l = []\n",
    "            for k, s in enumerate(p):\n",
    "                l.append(word_tokenize(s))\n",
    "            tdoc.append(l)\n",
    "        \n",
    "        for j, p in enumerate(tdoc):\n",
    "            for k, s in enumerate(p):\n",
    "                fv = [\n",
    "                    f\"sentence.doc={self.index}\",\n",
    "                    f\"sentence.paragraph={j}\",\n",
    "                    f\"sentence.position={k}\",\n",
    "                    f\"sentence.length={len(s)}\"\n",
    "                ]\n",
    "                \n",
    "                 # first tokens (and pos) in sentence\n",
    "                tokens_pos = self.get_first_pos(s, len(s))\n",
    "                for z, ps in enumerate(tokens_pos):\n",
    "                    fv.extend([\n",
    "                        f\"sentence.first_word[+{z}]={ps[0]}\",\n",
    "                        f\"sentence.first_pos[+{z}]={ps[1]}\",\n",
    "                        f\"sentence.word[+{z}]={ps[0].lower()}\",\n",
    "                        f\"sentence.word.isupper[+{z}]={ps[0].isupper()}\",\n",
    "                        f\"sentence.word.istitle[+{z}]={ps[0].istitle()}\",\n",
    "                        f\"sentence.word.isdigit[+{z}]={ps[0].isdigit()}\"\n",
    "                    ])    \n",
    "\n",
    "                    if(z > 0):\n",
    "                        fv.extend([\n",
    "                            f\"sentence.word.prev_pos[+{z}]={tokens_pos[z - 1][1]}\",\n",
    "                            f\"sentence.word.prev_isupper[+{z}]={tokens_pos[z - 1][0].isupper()}\",\n",
    "                            f\"sentence.word.prev_istitle[+{z}]={tokens_pos[z - 1][0].istitle()}\",\n",
    "                            f\"sentence.word.prev_isdigit[+{z}]={tokens_pos[z - 1][0].isdigit()}\"\n",
    "                        ])\n",
    "\n",
    "                    if(z < len(tokens_pos) - 1):\n",
    "                        fv.extend([\n",
    "                            f\"sentence.word.next_pos[+{z}]={tokens_pos[z + 1][1]}\",\n",
    "                            f\"sentence.word.next_isupper[+{z}]={tokens_pos[z + 1][0].isupper()}\",\n",
    "                            f\"sentence.word.next_istitle[+{z}]={tokens_pos[z + 1][0].istitle()}\",\n",
    "                            f\"sentence.word.next_isdigit[+{z}]={tokens_pos[z + 1][0].isdigit()}\"\n",
    "                        ])\n",
    "                \n",
    "                if k > 0: \n",
    "                    pass\n",
    "                else:\n",
    "                    fv.extend([\"BOS\"])\n",
    "                    \n",
    "                if k < len(p) - 1:\n",
    "                    pass\n",
    "                else: \n",
    "                    fv.extend([\"EOS\"])\n",
    "                \n",
    "                fvp.append(fv)\n",
    "\n",
    "        return fvp\n",
    "\n",
    "    # Used during the Classification Task\n",
    "\n",
    "    def get_feature_vector_for_sentence(self, idx):\n",
    "        fvp = []\n",
    "        tp = []\n",
    "        \n",
    "        p = self.doc[int(idx)]\n",
    "        \n",
    "        for k, s in enumerate(p):\n",
    "            tp.append(word_tokenize(s))\n",
    "            \n",
    "        for k, s in enumerate(tp):\n",
    "            fv = [\n",
    "                f\"sentence.doc={self.index}\",\n",
    "                f\"sentence.paragraph={idx}\",\n",
    "                f\"sentence.position={k}\",\n",
    "                f\"sentence.length={len(s)}\"\n",
    "            ]\n",
    "            \n",
    "             # first tokens (and pos) in sentence\n",
    "            tokens_pos = self.get_first_pos(s, len(s))\n",
    "            for z, ps in enumerate(tokens_pos):\n",
    "                fv.extend([\n",
    "                    f\"sentence.first_word[+{z}]={ps[0]}\",\n",
    "                    f\"sentence.first_pos[+{z}]={ps[1]}\",\n",
    "                    f\"sentence.word[+{z}]={ps[0].lower()}\",\n",
    "                    f\"sentence.word.isupper[+{z}]={ps[0].isupper()}\",\n",
    "                    f\"sentence.word.istitle[+{z}]={ps[0].istitle()}\",\n",
    "                    f\"sentence.word.isdigit[+{z}]={ps[0].isdigit()}\"\n",
    "                ])    \n",
    "                \n",
    "                if(z > 0):\n",
    "                    fv.extend([\n",
    "                        f\"sentence.word.prev_pos[+{z}]={tokens_pos[z - 1][1]}\",\n",
    "                        f\"sentence.word.prev_isupper[+{z}]={tokens_pos[z - 1][0].isupper()}\",\n",
    "                        f\"sentence.word.prev_istitle[+{z}]={tokens_pos[z - 1][0].istitle()}\",\n",
    "                        f\"sentence.word.prev_isdigit[+{z}]={tokens_pos[z - 1][0].isdigit()}\"\n",
    "                    ])\n",
    "                \n",
    "                \n",
    "                if(z < len(tokens_pos) - 1):\n",
    "                    fv.extend([\n",
    "                        f\"sentence.word.next_pos[+{z}]={tokens_pos[z + 1][1]}\",\n",
    "                        f\"sentence.word.next_isupper[+{z}]={tokens_pos[z + 1][0].isupper()}\",\n",
    "                        f\"sentence.word.next_istitle[+{z}]={tokens_pos[z + 1][0].istitle()}\",\n",
    "                        f\"sentence.word.next_isdigit[+{z}]={tokens_pos[z + 1][0].isdigit()}\"\n",
    "                    ])\n",
    "            \n",
    "            if k > 0: \n",
    "                pass\n",
    "            else:\n",
    "                fv.extend([\"BOS\"])\n",
    "\n",
    "            if k < len(p) - 1:\n",
    "                pass\n",
    "            else:\n",
    "                fv.extend([\"EOS\"])\n",
    "\n",
    "            fvp.append(fv)\n",
    "\n",
    "        return fvp\n",
    "    \n",
    "    def paragraph_length(self, p):\n",
    "        l = 0\n",
    "        \n",
    "        for s in p:\n",
    "            if(type(s) != list):\n",
    "                s = word_tokenize(s)\n",
    "            l += len(s)\n",
    "\n",
    "        return l\n",
    "    \n",
    "    def prev_paragraph_length(self, i):\n",
    "        if(i > 0):\n",
    "            return self.paragraph_length(self.doc[i-1])\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    def next_paragraph_length(self, i):\n",
    "        if(i < (len(self.doc) - 1)):\n",
    "            return self.paragraph_length(self.doc[i+1])\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def get_first_pos(self, t, n): \n",
    "        #  Using a Tagger. Which is part-of-speech\n",
    "        # tagger or POS-tagger.\n",
    "        tagged = nltk.pos_tag(t)\n",
    "        return [x for x in tagged][:n]\n",
    "\n",
    "    def get_last_pos(self, t, n): \n",
    "        #  Using a Tagger. Which is part-of-speech\n",
    "        # tagger or POS-tagger.\n",
    "        tagged = nltk.pos_tag(t)\n",
    "        \n",
    "        return [x for x in tagged][n:]\n",
    "    \n",
    "    def avg_sentence_length(self, p):\n",
    "        return round(self.paragraph_length(p) / len(p), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
